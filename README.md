# Mixtral: annotated code repository for understanding Mistral's Mixture of Experts (MoE)  

Original repository: https://github.com/mistralai/mistral-src  

This repository is created with the aim of enhancing understanding of the Mixture of Experts (MoE) concept within the Mistral model, also known as Mixtral. 
The repository contains annotated code for facilitating comprehension of the underlying mechanisms.
